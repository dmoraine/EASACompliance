# üöÄ D√©marrage Rapide - Chat MCP Client

Guide rapide pour utiliser le chat MCP avec les r√©gulations EASA.

## ‚ö° Installation en 3 √©tapes

### 1. Installer les d√©pendances

```bash
pip install -r requirements-chat.txt
```

### 2. Configurer les providers

```bash
# Copier le template
cp env.example .env

# √âditer avec vos cl√©s API
nano .env  # ou vim, code, etc.
```

**Configuration minimale pour tester** (avec Ollama local) :

```bash
# Pas besoin de cl√©s API pour Ollama !
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1:8b
```

### 3. Lancer le chat

```bash
# Avec Ollama (local, pas de cl√© n√©cessaire)
python chat_mcp.py --provider ollama

# Ou mode interactif pour choisir le provider
python chat_mcp.py
```

## üéØ Exemples d'utilisation

### Recherche simple

```
You: What are the flight time limitations?

# Le LLM va automatiquement appeler search_regulations()
# et vous donner une r√©ponse bas√©e sur les r√©gulations EASA
```

### R√©cup√©ration d'une r√©gulation sp√©cifique

```
You: Get me the full text of ORO.FTL.110

# Le LLM va appeler get_regulation("ORO.FTL.110")
```

### Validation de conformit√©

```
You: Validate this text: "Flight crew members must not exceed 900 hours in a calendar year"

# Le LLM va appeler validate_compliance() avec votre texte
```

### Questions complexes

```
You: What regulations apply to rest periods for long-haul flights?

# Le LLM peut combiner plusieurs appels d'outils pour r√©pondre
```

## üîß Configuration des providers

### OpenAI

```bash
OPENAI_API_KEY=sk-votre-cle-ici
OPENAI_MODEL=gpt-4o
```

### Ollama (Local - RECOMMAND√â pour tester)

```bash
# 1. Installer Ollama: https://ollama.ai/
# 2. T√©l√©charger un mod√®le
ollama pull llama3.1:8b

# 3. Lancer Ollama
ollama serve

# 4. Dans .env
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1:8b
```

### Hyperbolic

```bash
HYPERBOLIC_API_KEY=votre-cle-ici
HYPERBOLIC_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct
```

## üìã Commandes dans le chat

- `/quit` - Quitter
- `/tools` - Lister les outils MCP disponibles
- `/help` - Afficher l'aide
- `/provider` - Info sur le changement de provider

## ‚úÖ V√©rifier l'installation

```bash
python test_chat_setup.py
```

Ce script v√©rifie :
- ‚úÖ Imports et d√©pendances
- ‚úÖ Configuration des providers
- ‚úÖ Base de donn√©es EASA
- ‚úÖ Serveur MCP

## üé¨ Workflow typique

1. **Lancer le chat** : `python chat_mcp.py --provider ollama`
2. **Poser une question** : Le LLM appelle automatiquement les outils MCP si n√©cessaire
3. **Voir le r√©sultat** : R√©ponse stream√©e en temps r√©el avec citations r√©glementaires

## üêõ Probl√®mes courants

### "Database not found"

```bash
python easacompliance/scripts/build_embeddings.py \
    --xml "Easy Access Rules for Air Operations - February 2025 - xml.xml" \
    --db "easa_complete.db" \
    --clear
```

### "Provider not configured"

V√©rifiez votre fichier `.env` et assurez-vous d'avoir les bonnes cl√©s API.

### Ollama ne r√©pond pas

```bash
# V√©rifier qu'Ollama tourne
curl http://localhost:11434/v1/models

# Red√©marrer si n√©cessaire
ollama serve
```

## üìö Documentation compl√®te

- [CHAT_MCP_README.md](CHAT_MCP_README.md) - Documentation compl√®te
- [env.example](env.example) - Template de configuration
- [test_chat_setup.py](test_chat_setup.py) - Script de test

## üí° Astuce

Pour un test rapide sans cl√© API, utilisez **Ollama** en local :

```bash
# Installation en une ligne (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Lancer Ollama et t√©l√©charger un mod√®le
ollama serve &
ollama pull llama3.1:8b

# Tester le chat
python chat_mcp.py --provider ollama
```

Puis posez une question comme :
```
You: Search for regulations about crew rest requirements
```

Le LLM va automatiquement utiliser le serveur MCP pour chercher dans les r√©gulations EASA ! üéâ
